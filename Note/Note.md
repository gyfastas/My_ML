# Note

## Supervised Learning

### Multivariate Linear Regression

The gradient descent:
$$
\theta_j :=\theta_j -\alpha \frac 1m \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}).x_j^{(i)}
$$
**feature scaling ** and **mean normalization**

learning rate: if sufficient small , $j(\theta)$ will decrease on every iteration

**Normal Equation**



****



### Polynomial Regression



## Semi-Supervised Learning

## Unsupervised Learning

## Resources , program and tricks





